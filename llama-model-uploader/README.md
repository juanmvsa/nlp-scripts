# ü¶ô custom `llama 3.2` model uploader

a comprehensive python toolkit for converting and uploading fine-tuned [llama 3.2](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/) models to the [hugging face hub](https://huggingface.co/models).

this tool automatically converts from  original llama checkpoint format to huggingface-compatible format with full tokenizer support.

---

## ‚ú® features

- üîÑ **automatic format conversion**: converts llama checkpoint format to huggingface format
- üéØ **smart tokenizer handling**: works with or without sentencepiece (this library has some issues in arm-based apple systems), includes fallback mechanisms
- üì¶ **modular architecture**: clean, reusable components for different tasks
- üß† **memory efficient**: handles large models with intelligent memory management
- üîí **secure upload**: supports private repositories and token authentication
- üß™ **built-in testing**: validates model loading after upload
- üìä **progress tracking**: clear status messages and file size reporting
- üõ°Ô∏è **error resilience**: comprehensive error handling and fallback strategies
- ‚ö° **`uv`-powered**: uses `uv` for fast, reliable dependency management

---

## üöÄ quick start

### ‚Üí installation with [uv](https://huggingface.co/models) (recommended)

#### 0. install `uv` if you haven't already
```bash
curl -lssf https://astral.sh/uv/install.sh | sh
```

#### 1. clone this repository
```bash
git clone https://github.com/juanmvsa/nlp-scripts/
```

#### 2. create the virtual environment 
```bash
cd llama3-2-model-uploader
```

```bash
uv venv
```

```bash
source .venv/bin/activate  # on windows: .venv\scripts\activate
```

```bash
uv pip install -e .
```

#### 3. install with optional dependencies
```bash
uv pip install -e ".[all]"  # includes accelerate and sentencepiece
```

#### 4. set your `huggingface` token
```bash
export hf_token=your_huggingface_token_here
```

### ‚Üí alternative installation methods

#### ‚ó¶ install without `sentencepiece` (in case of build issues)
```bash
uv pip install -e ".[accelerate]"
```

#### ‚ó¶ development installation
```bash
uv pip install -e ".[dev]"
```

#### ‚ó¶ install specific extras
```bash
uv pip install -e ".[sentencepiece,accelerate]"
```

---

### ‚Üí basic usage

#### ‚ó¶ using the installed command
```bash
llama3-2-upload \
  --model_path ./your_llama32_model \
  --repo_name your-username/your-model-name
```

#### ‚ó¶ or using python directly
```bash
uv run python upload_model.py \
  --model_path ./your_llama32_model \
  --repo_name your-username/your-model-name
```

### ‚Üí full example
```bash
llama3-2-upload \
  --model_path ./my_finetuned_llama \
  --repo_name johndoe/llama32-finetuned-model \
  --private \
  --test
```

---

## üìÅ project structure

```
llama3-2-model-uploader/
‚îú‚îÄ‚îÄ upload_model.py          # üéØ main entry point
‚îú‚îÄ‚îÄ file_validator.py        # üîç file validation utilities
‚îú‚îÄ‚îÄ model_converter.py       # üîÑ format conversion logic
‚îú‚îÄ‚îÄ tokenizer_creator.py     # üé® tokenizer file creation
‚îú‚îÄ‚îÄ hf_uploader.py           # ‚¨ÜÔ∏è huggingface hub integration
‚îú‚îÄ‚îÄ model_tester.py          # üß™ model testing utilities
‚îú‚îÄ‚îÄ requirements.txt         # üì¶ python dependencies
‚îî‚îÄ‚îÄ readme.md               # üìñ documentation
```

---

## üìã requirements

### ‚Üí input files (required)

your `model` folder must contain:

```
your_model_folder/
‚îú‚îÄ‚îÄ consolidated.00.pth      # model weights (part 0)
‚îú‚îÄ‚îÄ consolidated.01.pth      # model weights (part 1)
‚îú‚îÄ‚îÄ ...                      # additional weight files
‚îú‚îÄ‚îÄ consolidated.xx.pth      # model weights (part x)
‚îú‚îÄ‚îÄ params.json              # model parameters
‚îú‚îÄ‚îÄ tokenizer.model          # sentencepiece tokenizer
‚îî‚îÄ‚îÄ checklist.chk           # optional (ignored during upload)
```

### ‚Üí generated files

the tool automatically creates:

- `config.json` - huggingface model configuration
- `pytorch_model.bin` - merged model weights
- `tokenizer_config.json` - tokenizer configuration
- `special_tokens_map.json` - special tokens mapping
- `tokenizer.json` - fast tokenizer (when possible)
- `readme.md` - model card with usage instructions

---

## üõ†Ô∏è command line options

| option | description | required |
|--------|-------------|----------|
| `--model_path` | path to your model folder | ‚úÖ |
| `--repo_name` | huggingface repo (`username/model-name`) | ‚úÖ |
| `--hf_token` | huggingface token (or set `hf_token` env var) | ‚ö†Ô∏è |
| `--private` | make repository private | ‚ùå |
| `--test` | test model loading after upload | ‚ùå |
| `--skip_conversion` | skip format conversion | ‚ùå |

---

## üîß advanced usage

### ‚Üí `uv` commands

#### ‚ó¶ environment management

##### 0. create and activate the virtual environment
```bash
uv venv
```

##### 1. linux/macos
```bash
source .venv/bin/activate  
```

##### or (windows)
```bash
.venv\scripts\activate
```

#### ‚ó¶ install project in development mode
```bash
uv pip install -e .
```

#### ‚ó¶ install with all optional dependencies
```bash
uv pip install -e ".[all]"
```

#### ‚ó¶ update dependencies
```bash
uv pip install --upgrade -e .
```

#### ‚ó¶ running the script

##### 0. using the installed command (recommended)
```bash
llama3.2-upload --model_path ./model --repo_name user/model
```

##### 1. using `uv run` (runs in an isolated environment)
```bash
uv run llama3-2-upload --model_path ./model --repo_name user/model
```

##### 2. using python directly
```bash
uv run python upload_model.py --model_path ./model --repo_name user/model
```

#### ‚ó¶ environment variables

##### set your `huggingface` token
```bash
export hf_token=hf_xxxxxxxxxxxxxxxxxxxx
```

##### run with the environment token
```bash
llama3-2-upload --model_path ./model --repo_name user/model
```

### ‚Üí programmatic usage

```python
from file_validator import validate_model_files
from model_converter import convert_llama_checkpoint
from hf_uploader import upload_model_to_hf

# validate files
if validate_model_files("./my_model"):
    # convert format
    convert_llama_checkpoint("./my_model")
    
    # upload to hub
    upload_model_to_hf("./my_model", "user/model", "hf_token")
```

### ‚Üí testing only

##### upload without testing
```bash
llama3-2-upload --model_path ./model --repo_name user/model
```

##### test an existing repository
```bash
uv run python -c "
from model_tester import test_model_loading_full
test_model_loading_full('user/model', 'your_token')
"
```

---

## üêç dependencies

this project uses [uv](https://github.com/astral-sh/uv) for fast, reliable dependency management.

### ‚Üí core requirements

all dependencies are defined in the `pyproject.toml` file:

```toml
dependencies = [
    "huggingface_hub>=0.19.0",    # hub integration
    "transformers>=4.43.0",       # model compatibility
    "torch>=2.0.0",              # pytorch backend
    "tokenizers>=0.15.0",        # tokenizer handling
    "requests>=2.25.0",          # http requests
    "tqdm>=4.64.0",              # progress bars
    "packaging>=20.0",           # version parsing
]
```

### ‚Üí optional dependencies

```toml
[project.optional-dependencies]
# performance improvements
accelerate = [
    "accelerate>=0.24.0",
    "safetensors>=0.4.0",
]

# tokenizer conversion (may have build issues on some platforms)
sentencepiece = [
    "sentencepiece>=0.1.97,<0.2.0",
]

# development tools
dev = [
    "black>=23.0.0",
    "isort>=5.12.0",
    "mypy>=1.5.0",
    "pytest>=7.4.0",
    "ruff>=0.0.290",
]

# all optional dependencies
all = ["llama3-2-model-uploader[accelerate,sentencepiece]"]
```

### ‚Üí `sentencepiece` installation issues

if you encounter build errors with `sentencepiece` on macos arm64:

#### ‚ó¶ option 1: use `conda`
```bash
conda install -c conda-forge sentencepiece
```

##### install without `sentencepiece` extra
```bash
uv pip install -e ".[accelerate]"  # 
```

#### ‚ó¶ option 2: use `homebrew` + `uv`
```bash
brew install protobuf
```

```bash
uv pip install -e ".[all]"
```

#### ‚ó¶ option 3: skip `sentencepiece` (the script will use the defined fallbacks)

##### install without `sentencepiece`
```
uv pip install -e ".[accelerate]"   
```

#### ‚ó¶ option 4: use the pre-built wheel
```
uv pip install sentencepiece --only-binary=sentencepiece
```

---

## üìñ usage examples

### ‚Üí example 1: basic upload
```bash
# simple upload with default settings
llama3-2-upload \
  --model_path ./llama32_finetuned \
  --repo_name myusername/llama32-chatbot
```

### ‚Üí example 2: private model with testing
```bash
# upload private model and test loading
llama3-2-upload \
  --model_path ./sensitive_model \
  --repo_name company/internal-model \
  --private \
  --test \
  --hf_token hf_xxxxxxxxxxxxxxxxxxxx
```

### ‚Üí example 3: skip conversion
```bash
# upload pre-converted model files
llama3-2-upload \
  --model_path ./already_converted_model \
  --repo_name user/converted-model \
  --skip_conversion
```

### ‚Üí example 4: using `uv run`
```bash
# run without installing (uses temporary environment)
uv run --with llama-model-uploader llama3-2-upload \
  --model_path ./model \
  --repo_name user/model
```

---

## üîç troubleshooting

### ‚Üí common issues

**‚ùå "missing required files"**
```bash
# ensure your model folder contains:
ls your_model_folder/
# should show: consolidated.*.pth, params.json, tokenizer.model
```

**‚ùå "token verification failed"**
```bash
# check your token has write permissions
export hf_token=hf_xxxxxxxxxxxxxxxxxxxx
python -c "from huggingface_hub import hfapi; print(hfapi().whoami())"
```

**‚ùå "`sentencepiece` build failed"**
```bash
# install via conda instead
conda install -c conda-forge sentencepiece
# or skip sentencepiece (tool will use fallbacks)
```

**‚ùå "out of memory during conversion"**
```bash
# the tool automatically handles this with fallbacks
# for very large models, it will copy the first checkpoint instead of merging
```

### ‚Üí debug mode

#### ‚ó¶ run with python's verbose output
```bash
uv run python -v upload_model.py --model_path ./model --repo_name user/model
```

#### ‚ó¶ check file sizes
```bash
uv run python -c "
from file_validator import list_model_files, print_file_summary
files = list_model_files('./your_model')
print_file_summary('./your_model', files)
"
```

#### ‚ó¶ run with uv verbose mode
```bash
uv --verbose pip install -e .
```

---

## üß™ testing your upload

after uploading, test your model:

```python
from transformers import autotokenizer, automodelforcausallm

# load your uploaded model
tokenizer = autotokenizer.from_pretrained("your-username/your-model")
model = automodelforcausallm.from_pretrained("your-username/your-model")

# test generation
prompt = "hello, i am a llama model"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=50)
response = tokenizer.decode(outputs[0], skip_special_tokens=true)
print(response)
```

---

## üìÑ license

this project is licensed under the mit license - see the [license](license) file for details.
